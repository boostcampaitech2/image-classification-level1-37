{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16afe1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed : 37\n",
      "device : cuda:0\n",
      "_CudaDeviceProperties(name='Tesla V100-PCIE-32GB', major=7, minor=0, total_memory=32510MB, multi_processor_count=80)\n",
      "root : /opt/ml\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from adamp import AdamP\n",
    "from tqdm import tqdm, notebook\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# random seed\n",
    "seed = 37\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "print(f'seed : {seed}')\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'device : {device}')\n",
    "print(torch.cuda.get_device_properties(device))\n",
    "\n",
    "# root\n",
    "root = os.getcwd()\n",
    "print(f'root : {root}')\n",
    "\n",
    "# Training Name\n",
    "name = 'restoration_final'\n",
    "if not os.path.isdir(f'data_file/{name}') :\n",
    "    os.chdir(os.path.join(root, 'data_file'))\n",
    "    os.mkdir(f'{name}')\n",
    "    os.chdir(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cde79206",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('input/data/train/images')\n",
    "image_dirs = [str(x) for x in list(path.glob('*')) if '._' not in str(x)]\n",
    "# 'input/data/train/images/003277_female_Asian_19' 이런 형태 나옴\n",
    "\n",
    "image_dirs = np.array(image_dirs)\n",
    "\n",
    "# 나이와 성별 구분이 문제니까 이 두 개를 기준으로 나눠서 stratified_kfold하면 inbalance를 조금 방지할 수 있지 않을까?\n",
    "# 나이 성별 정보면 이용해서 데이터 나누기\n",
    "# 나이가 60세 이상 정보가 너무 부족하니까 58 기준으로 나눠서 진행해보기\n",
    "def label_fold(image_dirs):\n",
    "    stratified_kfold_label = []\n",
    "    for image_dir in image_dirs :\n",
    "        cnt = 0\n",
    "        if 'female' in image_dir : cnt += 3\n",
    "        else : cnt += 0 \n",
    "        \n",
    "        age = int(image_dir.split('_')[3][:2])\n",
    "        if age < 30 : cnt += 0\n",
    "        elif age < 58 : cnt += 1\n",
    "        else : cnt += 2\n",
    "        stratified_kfold_label.append(cnt)\n",
    "    stratified_kfold_label = np.array(stratified_kfold_label)\n",
    "    stratified_kfold = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)\n",
    "    # Stratified K-Fold는 층화된 folds를 반환하는 기존 K-Fold의 변형된 방식. 각 집합에는 전체 집합과 거의 동일하게 클래스의 표본 비율이 포함된다. 불균형 클래스의 경우 사\n",
    "    # train에 2700개 중에서 4/5가 들어가고 valid에 1/5가 들어간다. 이걸 train할 때 다섯번 반복하면 된다.\n",
    "    fold_list = []\n",
    "    for train_data, valid_data in stratified_kfold.split(image_dirs, stratified_kfold_label) : # split(x,y) x training data, y target\n",
    "        fold_list.append({'train':train_data, 'valid':valid_data})\n",
    "    return fold_list\n",
    "\n",
    "fold_list = label_fold(image_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbac6573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Mask까지 포함해서, 다시 라벨링하기'\n",
    "def label_func(image_paths) :\n",
    "        cnt = 0\n",
    "        if 'normal' in image_paths : cnt += 12\n",
    "        elif 'incorrect_mask' in image_paths : cnt += 6\n",
    "        else : cnt += 0\n",
    "\n",
    "        if 'female' in image_paths : cnt += 3\n",
    "        else : cnt += 0\n",
    "\n",
    "        age = int(image_paths.split('_')[3][:2])\n",
    "        if age < 30 : cnt += 0\n",
    "        elif age < 58 : cnt += 1\n",
    "        else : cnt += 2\n",
    "\n",
    "        return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "458581b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class MaskDataset(Dataset) :\n",
    "    # path input/data/train/images/003277_female_Asian_19/mask3.jpg 이런 식으로 들어옴\n",
    "    def __init__(self, image_paths, transform, augment = None, training = False):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        self.training = training\n",
    "        \n",
    "    def __len__(self) :\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = np.array(Image.open(self.image_paths[idx]))\n",
    "        \n",
    "        if self.augment: # augmentation하는 경우에\n",
    "            image = self.transform(self.augment(image = image)['image'])\n",
    "        else:    \n",
    "            image = self.transform(image)\n",
    "            \n",
    "        if self.training : # 트레이닝하는 경우\n",
    "            label = label_func(self.image_paths[idx])\n",
    "            return {'image' : image, 'label' : label}\n",
    "            \n",
    "        else:\n",
    "            return {'image' : image}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc884bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform\n",
    "'''\n",
    "transforms.ToPILImage() - csv 파일로 데이터셋을 받을 경우, PIL image로 바꿔준다.\n",
    "transforms.CenterCrop(size) - 가운데 부분을 size 크기로 자른다.\n",
    "transforms.Grayscale(num_output_channels=1) - grayscale로 변환한다.\n",
    "transforms.RandomAffine(degrees) - 랜덤으로 affine 변형을 한다.\n",
    "transforms.RandomCrop(size) -이미지를 랜덤으로 아무데나 잘라 size 크기로 출력한다.\n",
    "transforms.RandomResizedCrop(size) - 이미지 사이즈를 size로 변경한다\n",
    "transforms.Resize(size) - 이미지 사이즈를 size로 변경한다\n",
    "transforms.RandomRotation(degrees) 이미지를 랜덤으로 degrees 각도로 회전한다.\n",
    "transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333)) - 이미지를 랜덤으로 변형한다.\n",
    "transforms.RandomVerticalFlip(p=0.5) - 이미지를 랜덤으로 수직으로 뒤집는다. p =0이면 뒤집지 않는다.\n",
    "transforms.RandomHorizontalFlip(p=0.5) - 이미지를 랜덤으로 수평으로 뒤집는다.\n",
    "transforms.ToTensor() - 이미지 데이터를 tensor로 바꿔준다.\n",
    "transforms.Normalize(mean, std, inplace=False) - 이미지를 정규화한다.\n",
    "'''\n",
    "train_transform = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.CenterCrop([300,250]),\n",
    "    T.RandomHorizontalFlip(0.5),\n",
    "    T.RandomRotation(15),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=(0.548, 0.504, 0.479), std=(0.237, 0.247, 0.246))\n",
    "])\n",
    "\n",
    "\n",
    "valid_transform = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.CenterCrop([300,250]),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=(0.56, 0.51, 0.48), std=(0.22, 0.24, 0.25))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1812210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class MyModel(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        self.model_name = EfficientNet.from_pretrained('efficientnet-b3', \n",
    "                                                in_channels=3, \n",
    "                                                num_classes=18) # weight가져오고 num_classes(두번째 파라미터로 학습시키는 class 수)\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        x = F.relu(self.model_name(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27dea0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "batch_size = 32\n",
    "lr = 1e-4\n",
    "epochs = 10\n",
    "\n",
    "counter = 0\n",
    "patience = 10\n",
    "accumulation_steps = 2\n",
    "best_val_acc = 0\n",
    "best_val_loss = np.inf\n",
    "lr_decay_step = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cf42e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold number 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/473 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 10: 100%|██████████| 473/473 [02:37<00:00,  3.00batch/s, Train=1, f1=0.576, loss=0.899]\n",
      "Epoch 1 / 10: 100%|██████████| 473/473 [00:14<00:00, 32.46batch/s, Valid=1, f1=0.73, loss=0.448] \n",
      "Epoch 2 / 10:   9%|▊         | 41/473 [00:14<02:29,  2.89batch/s, Train=2, f1=0.747, loss=0.357]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-14118a36564a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;31m# print f1 score and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/adamp/adamp.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mwd_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                     \u001b[0mperturb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_projection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperturb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'delta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wd_ratio'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;31m# Weight decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/adamp/adamp.py\u001b[0m in \u001b[0;36m_projection\u001b[0;34m(self, p, grad, perturb, delta, wd_ratio, eps)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mcosine_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mcosine_sim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mview_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0mp_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mview_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpand_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mperturb\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mp_n\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mview_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_n\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mperturb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpand_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training and Validating\n",
    "\n",
    "folds_index = [1, 2, 3, 4, 5] # 총 5개\n",
    "\n",
    "for fold in folds_index :\n",
    "    print(f'Fold number {fold}')\n",
    "    min_loss = 3\n",
    "    early_stop = 0\n",
    "\n",
    "    # -- kfold를 이용해서 dataset을 만들기 위한 경로 리스트 만들기\n",
    "    train_image_paths, valid_image_paths = [], []\n",
    "    for train_dir in image_dirs[fold_list[fold-1]['train']] :\n",
    "        train_image_paths.extend(glob(train_dir+'/*'))\n",
    "    for valid_dir in image_dirs[fold_list[fold-1]['valid']] :\n",
    "        valid_image_paths.extend(glob(valid_dir+'/*'))\n",
    "    # -- dataset\n",
    "    train_dataset = MaskDataset(train_image_paths, train_transform, training=True)\n",
    "    valid_dataset = MaskDataset(valid_image_paths, valid_transform, training=True)\n",
    "    # -- data_loader\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=3)\n",
    "    valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size//4, shuffle=True, num_workers=3)\n",
    "    \n",
    "    # -- model\n",
    "    model = MyModel()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # -- loss & metric\n",
    "    optimizer = AdamP(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    scheduler = StepLR(optimizer, lr_decay_step, gamma=0.5)\n",
    "\n",
    "    # -- logging\n",
    "    logger = SummaryWriter(log_dir=f\"data_file/cv{fold}_{name}\")\n",
    "    for epoch in range(epochs) :\n",
    "        \n",
    "        # -- Train start\n",
    "        with tqdm(train_loader, total=train_loader.__len__(), unit='batch') as train_depth :\n",
    "            train_f1 = []\n",
    "            train_loss = []\n",
    "            for sample in train_depth :\n",
    "                train_depth.set_description(f'Epoch {epoch+1} / {epochs}')\n",
    "                images = sample['image'].float().to(device)\n",
    "                labels = sample['label'].long().to(device)\n",
    "                \n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                pred = model(images)\n",
    "                loss = criterion(pred, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # print f1 score and loss\n",
    "                train_f1.append(f1_score(labels.cpu().detach().float(), torch.argmax(pred.cpu().detach(), 1), average='macro'))\n",
    "                train_loss.append(loss.item())\n",
    "                train_depth.set_postfix(f1=np.mean(train_f1), loss=np.mean(train_loss), Train=epoch+1)\n",
    "        \n",
    "        # --  Validation start\n",
    "\n",
    "        with tqdm(valid_loader, total=valid_loader.__len__(), unit='batch') as valid_depth :\n",
    "            valid_f1 = []\n",
    "            valid_loss = []\n",
    "            for sample in valid_depth :\n",
    "                valid_depth.set_description(f'Epoch {epoch+1} / {epochs}')\n",
    "                imgs = sample['image'].float().to(device)\n",
    "                labels = sample['label'].long().to(device)\n",
    "                \n",
    "                model.eval()\n",
    "                optimizer.zero_grad()\n",
    "                with torch.no_grad() : \n",
    "                    pred = model(imgs)\n",
    "                    loss = criterion(pred, labels)\n",
    "\n",
    "                'f1 score와 loss가 표시됩니다.'\n",
    "                valid_f1.append(f1_score(labels.cpu().detach().float(), torch.argmax(pred.cpu().detach(), 1), average='macro'))\n",
    "                valid_loss.append(loss.item())\n",
    "                valid_depth.set_postfix(f1=np.mean(valid_f1), loss=np.mean(valid_loss), Valid=epoch+1)\n",
    "        \n",
    "        # Loss가 더 낮아지면 해당 Model을 저장하고 학습이 5번 이상 진전이 없다면 조기종료'\n",
    "        if np.mean(valid_loss) < min_loss :\n",
    "            min_loss = np.mean(valid_loss)\n",
    "            early_stop = 0\n",
    "            for f in glob(f'data_file/{name}/{fold}fold_*{name}.ckpt') :\n",
    "                open(f, 'w').close()\n",
    "                os.remove(f)\n",
    "            torch.save(model.state_dict(), f'data_file/{name}/{fold}fold_{epoch+1}epoch_{np.mean(valid_loss):2.4f}_{name}.ckpt')\n",
    "        else :\n",
    "            early_stop += 1\n",
    "            if early_stop >= 5 : break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9136db78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/394 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 394/394 [01:20<00:00,  4.87batch/s]\n",
      "  0%|          | 0/394 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 394/394 [01:28<00:00,  4.47batch/s]\n",
      "  0%|          | 0/394 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 394/394 [01:20<00:00,  4.89batch/s]\n",
      "  0%|          | 0/394 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 394/394 [01:20<00:00,  4.89batch/s]\n",
      "  0%|          | 0/394 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 394/394 [01:19<00:00,  4.93batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "test_dir = '/opt/ml/input/data/eval'\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = [os.path.join(test_dir, 'images')\n",
    "\n",
    "test_image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "test_dataset = MaskDataset(test_image_paths, valid_transform, training = False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "all_predictions = []\n",
    "for best_model in glob(f'data_file/{name}/*{name}.ckpt') :\n",
    "    model = MyModel()\n",
    "    model.load_state_dict(torch.load(best_model))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    prediction_array=[]\n",
    "    \n",
    "    with tqdm(test_loader, total=test_loader.__len__(), unit='batch') as test_depth :\n",
    "        for sample in test_depth :\n",
    "            imgs = sample['image'].float().to(device)\n",
    "            pred = model(imgs)\n",
    "            pred = pred.cpu().detach().numpy()\n",
    "            prediction_array.extend(pred)\n",
    "    \n",
    "    all_predictions.append(np.array(prediction_array)[...,np.newaxis])\n",
    "submission['ans'] = np.argmax(np.mean(np.concatenate(all_predictions, axis=2), axis=2), axis=1)\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(f'data_file/{name}/{name}.csv', index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b88980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
