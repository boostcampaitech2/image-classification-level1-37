{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "981f0a60-d082-42e2-a493-f8160d0ca6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, random_split, TensorDataset, SubsetRandomSampler\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from adamp import AdamP\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "814308d9-60ac-4c1e-a7d5-ce40d13ba63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed : 37\n",
      "device : cuda:0\n",
      "_CudaDeviceProperties(name='Tesla V100-PCIE-32GB', major=7, minor=0, total_memory=32510MB, multi_processor_count=80)\n"
     ]
    }
   ],
   "source": [
    "# seed\n",
    "'''\n",
    "동일한 조건으로 학습을 할 때, 동일한 결과를 얻기 위해 seed를 고정시킵니다.\n",
    "'''\n",
    "seed = 37\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "print(f'seed : {seed}')\n",
    "\n",
    "# device setting\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'device : {device}')\n",
    "print(torch.cuda.get_device_properties(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e819f79-30ea-494c-b14e-7ec098ca67f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameter\n",
    "batch_size = 8\n",
    "num_workers = 4\n",
    "num_classes = 3\n",
    "\n",
    "num_epochs = 1  # 학습할 epoch의 수\n",
    "lr = 0.0001\n",
    "lr_decay_step = 10\n",
    "criterion_name = 'cross_entropy' # loss의 이름\n",
    "\n",
    "train_log_interval = 20  # logging할 iteration의 주기\n",
    "name = \"02_model_results\"  # 결과를 저장하는 폴더의 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ac411bb-ef74-47a7-8839-9fd4d589b7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaskDataset 만들기\n",
    "class MaskDataset(Dataset):\n",
    "    def __init__(self,img_path,label_path,transform=True):\n",
    "        self.image = self.load_image(img_path)\n",
    "\n",
    "        self.transform = transform\n",
    "        self.label_path = label_path\n",
    "        #self.age, self.gender, self.mask = self.load_label(label_path)\n",
    "        self.label = self.load_label(label_path)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        #image, age, gender, mask = Image.open(self.image[idx]), self.age[idx], self.gender[idx], self.mask[idx]\n",
    "        image, label= Image.open(self.image[idx]), self.label[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        #return image, age, gender, mask\n",
    "        return image, label\n",
    "    def __len__(self):\n",
    "            return len(self.label)\n",
    "\n",
    "    def load_image(self,paths):\n",
    "        img_lst = []\n",
    "        for dic in os.listdir(paths):\n",
    "            if '._' in dic or 'ipynb_checkpoints' in dic:\n",
    "                continue\n",
    "            dir_path = paths + '/'+ dic\n",
    "            for image in os.listdir(dir_path):\n",
    "                if '._' in image or 'ipynb_checkpoints' in image:\n",
    "                    continue\n",
    "                image_path = dir_path + '/' + image \n",
    "                img_lst.append(image_path)\n",
    "        return img_lst\n",
    "    \n",
    "    def load_label(self, paths):\n",
    "        df = pd.read_csv(os.path.join(paths, \"train_with_labels.csv\"))\n",
    "        #return df['age'], df['gender'], df['mask']\n",
    "        return df['age']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60f5e069-e614-4b4a-b7b8-37657bbdd8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터로더 만들기\n",
    "def getDataloader(dataset, train_idx, valid_idx, batch_size, num_workers):\n",
    "    # 인자로 전달받은 dataset에서 train_idx에 해당하는 Subset 추출\n",
    "    train_set = torch.utils.data.Subset(dataset,\n",
    "                                        indices=train_idx)\n",
    "    # 인자로 전달받은 dataset에서 valid_idx에 해당하는 Subset 추출\n",
    "    val_set = torch.utils.data.Subset(dataset,\n",
    "                                      indices=valid_idx)\n",
    "    \n",
    "    # 추출된 Train Subset으로 DataLoader 생성\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=True,\n",
    "        shuffle=True\n",
    "    )\n",
    "    # 추출된 Valid Subset으로 DataLoader 생성\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_set,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=True,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # 생성한 DataLoader 반환\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5a8c570-a91a-4585-8547-b3bbd9d1b895",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/opt/ml/input/data/train/images'\n",
    "label_path = '/opt/ml/input/data/train'\n",
    "transform = T.Compose([\n",
    "    T.Resize((512,384)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "]) \n",
    "DATA = MaskDataset(img_path,label_path,transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3478a518-b843-48cf-9d6b-590f05d03665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네트워크 필요 입력 채널 개수 3\n",
      "네트워크 출력 채널 개수 (예측 class type 개수) 1000\n"
     ]
    }
   ],
   "source": [
    "resnet18_pretrained = torchvision.models.resnet18()\n",
    "\n",
    "print(\"네트워크 필요 입력 채널 개수\", resnet18_pretrained.conv1.weight.shape[1])\n",
    "print(\"네트워크 출력 채널 개수 (예측 class type 개수)\", resnet18_pretrained.fc.weight.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cb61430-6c98-44d4-9b5b-1d862407c56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "target_model = resnet18_pretrained\n",
    "\n",
    "os.makedirs(os.path.join(os.getcwd(), 'results', name), exist_ok=True)\n",
    "\n",
    "# 5-fold Stratified KFold 5개의 fold를 형성하고 5번 Cross Validation을 진행합니다.\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "counter = 0\n",
    "patience = 10\n",
    "accumulation_steps = 2\n",
    "best_val_acc = 0\n",
    "best_val_loss = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c35c474-97ce-4739-b915-ddd293b80485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/1](20/1890) || training loss 6.832 || training accuracy 2.50% || lr [0.0001]\n",
      "Epoch[0/1](40/1890) || training loss 6.039 || training accuracy 8.12% || lr [0.0001]\n",
      "Epoch[0/1](60/1890) || training loss 5.468 || training accuracy 7.50% || lr [0.0001]\n",
      "Epoch[0/1](80/1890) || training loss 4.753 || training accuracy 5.62% || lr [0.0001]\n",
      "Epoch[0/1](100/1890) || training loss 4.027 || training accuracy 9.38% || lr [0.0001]\n",
      "Epoch[0/1](120/1890) || training loss 3.923 || training accuracy 8.75% || lr [0.0001]\n",
      "Epoch[0/1](140/1890) || training loss 3.465 || training accuracy 11.88% || lr [0.0001]\n",
      "Epoch[0/1](160/1890) || training loss 3.511 || training accuracy 9.38% || lr [0.0001]\n",
      "Epoch[0/1](180/1890) || training loss 3.488 || training accuracy 11.25% || lr [0.0001]\n",
      "Epoch[0/1](200/1890) || training loss 3.33 || training accuracy 14.37% || lr [0.0001]\n",
      "Epoch[0/1](220/1890) || training loss 3.231 || training accuracy 15.00% || lr [0.0001]\n",
      "Epoch[0/1](240/1890) || training loss 3.294 || training accuracy 12.50% || lr [0.0001]\n",
      "Epoch[0/1](260/1890) || training loss 3.335 || training accuracy 13.75% || lr [0.0001]\n",
      "Epoch[0/1](280/1890) || training loss 3.487 || training accuracy 11.88% || lr [0.0001]\n",
      "Epoch[0/1](300/1890) || training loss 3.126 || training accuracy 13.12% || lr [0.0001]\n",
      "Epoch[0/1](320/1890) || training loss 3.298 || training accuracy 9.38% || lr [0.0001]\n",
      "Epoch[0/1](340/1890) || training loss 3.231 || training accuracy 14.37% || lr [0.0001]\n",
      "Epoch[0/1](360/1890) || training loss 3.235 || training accuracy 13.75% || lr [0.0001]\n",
      "Epoch[0/1](380/1890) || training loss 3.301 || training accuracy 16.25% || lr [0.0001]\n",
      "Epoch[0/1](400/1890) || training loss 3.185 || training accuracy 19.38% || lr [0.0001]\n",
      "Epoch[0/1](420/1890) || training loss 3.383 || training accuracy 13.12% || lr [0.0001]\n",
      "Epoch[0/1](440/1890) || training loss 3.324 || training accuracy 13.75% || lr [0.0001]\n",
      "Epoch[0/1](460/1890) || training loss 3.231 || training accuracy 18.12% || lr [0.0001]\n",
      "Epoch[0/1](480/1890) || training loss 3.262 || training accuracy 13.75% || lr [0.0001]\n",
      "Epoch[0/1](500/1890) || training loss 3.196 || training accuracy 11.88% || lr [0.0001]\n",
      "Epoch[0/1](520/1890) || training loss 3.168 || training accuracy 9.38% || lr [0.0001]\n",
      "Epoch[0/1](540/1890) || training loss 3.193 || training accuracy 18.75% || lr [0.0001]\n",
      "Epoch[0/1](560/1890) || training loss 3.218 || training accuracy 11.25% || lr [0.0001]\n",
      "Epoch[0/1](580/1890) || training loss 3.126 || training accuracy 15.62% || lr [0.0001]\n",
      "Epoch[0/1](600/1890) || training loss 3.164 || training accuracy 20.00% || lr [0.0001]\n",
      "Epoch[0/1](620/1890) || training loss 3.344 || training accuracy 13.12% || lr [0.0001]\n",
      "Epoch[0/1](640/1890) || training loss 3.295 || training accuracy 18.75% || lr [0.0001]\n",
      "Epoch[0/1](660/1890) || training loss 3.077 || training accuracy 18.75% || lr [0.0001]\n",
      "Epoch[0/1](680/1890) || training loss 3.151 || training accuracy 19.38% || lr [0.0001]\n",
      "Epoch[0/1](700/1890) || training loss 3.313 || training accuracy 16.88% || lr [0.0001]\n",
      "Epoch[0/1](720/1890) || training loss 3.206 || training accuracy 14.37% || lr [0.0001]\n",
      "Epoch[0/1](740/1890) || training loss 3.245 || training accuracy 11.88% || lr [0.0001]\n",
      "Epoch[0/1](760/1890) || training loss 3.171 || training accuracy 14.37% || lr [0.0001]\n",
      "Epoch[0/1](780/1890) || training loss 3.064 || training accuracy 16.25% || lr [0.0001]\n",
      "Epoch[0/1](800/1890) || training loss 3.217 || training accuracy 10.62% || lr [0.0001]\n",
      "Epoch[0/1](820/1890) || training loss 3.148 || training accuracy 14.37% || lr [0.0001]\n",
      "Epoch[0/1](840/1890) || training loss 3.147 || training accuracy 13.75% || lr [0.0001]\n",
      "Epoch[0/1](860/1890) || training loss 3.261 || training accuracy 15.00% || lr [0.0001]\n",
      "Epoch[0/1](880/1890) || training loss 3.066 || training accuracy 16.88% || lr [0.0001]\n",
      "Epoch[0/1](900/1890) || training loss 3.22 || training accuracy 16.88% || lr [0.0001]\n",
      "Epoch[0/1](920/1890) || training loss 3.169 || training accuracy 16.25% || lr [0.0001]\n",
      "Epoch[0/1](940/1890) || training loss 3.178 || training accuracy 17.50% || lr [0.0001]\n",
      "Epoch[0/1](960/1890) || training loss 3.237 || training accuracy 20.62% || lr [0.0001]\n",
      "Epoch[0/1](980/1890) || training loss 3.231 || training accuracy 17.50% || lr [0.0001]\n",
      "Epoch[0/1](1000/1890) || training loss 3.271 || training accuracy 11.88% || lr [0.0001]\n",
      "Epoch[0/1](1020/1890) || training loss 3.106 || training accuracy 19.38% || lr [0.0001]\n",
      "Epoch[0/1](1040/1890) || training loss 3.219 || training accuracy 10.62% || lr [0.0001]\n",
      "Epoch[0/1](1060/1890) || training loss 3.165 || training accuracy 15.00% || lr [0.0001]\n",
      "Epoch[0/1](1080/1890) || training loss 3.224 || training accuracy 11.25% || lr [0.0001]\n",
      "Epoch[0/1](1100/1890) || training loss 3.248 || training accuracy 11.88% || lr [0.0001]\n",
      "Epoch[0/1](1120/1890) || training loss 3.105 || training accuracy 13.12% || lr [0.0001]\n",
      "Epoch[0/1](1140/1890) || training loss 3.158 || training accuracy 15.62% || lr [0.0001]\n",
      "Epoch[0/1](1160/1890) || training loss 3.298 || training accuracy 11.88% || lr [0.0001]\n",
      "Epoch[0/1](1180/1890) || training loss 3.291 || training accuracy 10.00% || lr [0.0001]\n",
      "Epoch[0/1](1200/1890) || training loss 3.217 || training accuracy 11.25% || lr [0.0001]\n",
      "Epoch[0/1](1220/1890) || training loss  3.2 || training accuracy 15.00% || lr [0.0001]\n",
      "Epoch[0/1](1240/1890) || training loss 2.995 || training accuracy 16.25% || lr [0.0001]\n",
      "Epoch[0/1](1260/1890) || training loss 3.283 || training accuracy 11.88% || lr [0.0001]\n",
      "Epoch[0/1](1280/1890) || training loss 3.142 || training accuracy 14.37% || lr [0.0001]\n",
      "Epoch[0/1](1300/1890) || training loss 3.133 || training accuracy 15.00% || lr [0.0001]\n",
      "Epoch[0/1](1320/1890) || training loss 3.216 || training accuracy 18.12% || lr [0.0001]\n",
      "Epoch[0/1](1340/1890) || training loss 3.23 || training accuracy 15.62% || lr [0.0001]\n",
      "Epoch[0/1](1360/1890) || training loss 3.13 || training accuracy 15.62% || lr [0.0001]\n",
      "Epoch[0/1](1380/1890) || training loss 3.243 || training accuracy 14.37% || lr [0.0001]\n",
      "Epoch[0/1](1400/1890) || training loss 3.213 || training accuracy 10.62% || lr [0.0001]\n",
      "Epoch[0/1](1420/1890) || training loss 3.116 || training accuracy 16.25% || lr [0.0001]\n",
      "Epoch[0/1](1440/1890) || training loss 3.435 || training accuracy 8.12% || lr [0.0001]\n",
      "Epoch[0/1](1460/1890) || training loss 3.134 || training accuracy 21.25% || lr [0.0001]\n",
      "Epoch[0/1](1480/1890) || training loss 3.137 || training accuracy 17.50% || lr [0.0001]\n",
      "Epoch[0/1](1500/1890) || training loss 3.255 || training accuracy 14.37% || lr [0.0001]\n",
      "Epoch[0/1](1520/1890) || training loss 3.247 || training accuracy 11.88% || lr [0.0001]\n",
      "Epoch[0/1](1540/1890) || training loss 3.184 || training accuracy 10.00% || lr [0.0001]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ca4656937168>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 학습 진행\n",
    "for i, (train_idx, valid_idx) in enumerate(skf.split(DATA.image, DATA.label)):\n",
    "    \n",
    "    # 생성한 Train, Valid Index를 getDataloader 함수에 전달해 train/valid DataLoader를 생성합니다.\n",
    "    # 생성한 train, valid DataLoader로 이전과 같이 모델 학습을 진행합니다. \n",
    "    train_loader, val_loader = getDataloader(DATA, train_idx, valid_idx, batch_size, num_workers)\n",
    "\n",
    "    # -- model\n",
    "    model = resnet18_pretrained\n",
    "    if torch.cuda.is_available():\n",
    "        model.to(device)\n",
    "    # -- loss & metric\n",
    "    criterion = torch.nn.CrossEntropyLoss() # 분류 학습 때 많이 사용되는 Cross entropy loss를 objective function으로 사용 - https://en.wikipedia.org/wiki/Cross_entropy\n",
    "    optimizer = Adam(target_model.parameters(), lr=lr) # weight 업데이트를 위한 optimizer를 Adam으로 사용함\n",
    "\n",
    "    scheduler = StepLR(optimizer, lr_decay_step, gamma=0.5)\n",
    "\n",
    "    # -- logging\n",
    "    logger = SummaryWriter(log_dir=f\"results/cv{i}_{name}\")\n",
    "    for epoch in range(num_epochs):\n",
    "        # train loop\n",
    "        model.train()\n",
    "        loss_value = 0\n",
    "        matches = 0\n",
    "        for idx, train_batch in enumerate(train_loader):\n",
    "            inputs, labels = train_batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outs = model(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "            loss = criterion(outs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "             # -- Gradient Accumulation\n",
    "            if (idx+1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            loss_value += loss.item()\n",
    "            matches += (preds == labels).sum().item()\n",
    "            if (idx + 1) % train_log_interval == 0:\n",
    "                train_loss = loss_value / train_log_interval\n",
    "                train_acc = matches / batch_size / train_log_interval\n",
    "                current_lr = scheduler.get_last_lr()\n",
    "                print(\n",
    "                    f\"Epoch[{epoch}/{num_epochs}]({idx + 1}/{len(train_loader)}) || \"\n",
    "                    f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n",
    "                )\n",
    "\n",
    "                loss_value = 0\n",
    "                matches = 0\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # val loop\n",
    "        with torch.no_grad():\n",
    "            print(\"Calculating validation results...\")\n",
    "            model.eval()\n",
    "            val_loss_items = []\n",
    "            val_acc_items = []\n",
    "            for val_batch in val_loader:\n",
    "                inputs, labels = val_batch\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outs = model(inputs)\n",
    "                preds = torch.argmax(outs, dim=-1)\n",
    "\n",
    "                loss_item = criterion(outs, labels).item()\n",
    "                acc_item = (labels == preds).sum().item()\n",
    "                val_loss_items.append(loss_item)\n",
    "                val_acc_items.append(acc_item)\n",
    "\n",
    "            val_loss = np.sum(val_loss_items) / len(val_loader)\n",
    "            val_acc = np.sum(val_acc_items) / len(valid_idx)\n",
    "\n",
    "            # Callback1: validation accuracy가 향상될수록 모델을 저장합니다.\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "            if val_acc > best_val_acc:\n",
    "                print(\"New best model for val accuracy! saving the model..\")\n",
    "                torch.save(model.state_dict(), f\"results/{name}/{epoch:03}_accuracy_{val_acc:4.2%}.ckpt\")\n",
    "                best_val_acc = val_acc\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "            # Callback2: patience 횟수 동안 성능 향상이 없을 경우 학습을 종료시킵니다.\n",
    "            if counter > patience:\n",
    "                print(\"Early Stopping...\")\n",
    "                break\n",
    "\n",
    "\n",
    "            print(\n",
    "                f\"[Val] acc : {val_acc:4.2%}, loss: {val_loss:4.2} || \"\n",
    "                f\"best acc : {best_val_acc:4.2%}, best loss: {best_val_loss:4.2}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3fd443-de9b-4eb2-b193-ef226572650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform, resize):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = T.Compose([\n",
    "    T.Resize((512,384)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912d324f-0d45-4788-bd08-db037ae40dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, num_classes: int = 1000):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dbd5f6-14aa-48d5-a36d-b6d60b0be29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc54a07a-b0c2-4bb0-be3d-569ffa3aaa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "test_dir = '/opt/ml/input/data/eval'\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "transform = T.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])\n",
    "dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')\n",
    "model = MyModel(num_classes=18).to(device)\n",
    "model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc122aa8-dc84-45ac-aea5-6f322f11cc85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
