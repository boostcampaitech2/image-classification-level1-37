{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "981f0a60-d082-42e2-a493-f8160d0ca6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, random_split, TensorDataset, SubsetRandomSampler\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from adamp import AdamP\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "814308d9-60ac-4c1e-a7d5-ce40d13ba63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed : 42\n",
      "device : cuda:0\n",
      "_CudaDeviceProperties(name='Tesla V100-PCIE-32GB', major=7, minor=0, total_memory=32510MB, multi_processor_count=80)\n"
     ]
    }
   ],
   "source": [
    "# seed\n",
    "'''\n",
    "동일한 조건으로 학습을 할 때, 동일한 결과를 얻기 위해 seed를 고정시킵니다.\n",
    "'''\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "print(f'seed : {seed}')\n",
    "\n",
    "# device setting\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'device : {device}')\n",
    "print(torch.cuda.get_device_properties(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e819f79-30ea-494c-b14e-7ec098ca67f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameter\n",
    "batch_size = 16\n",
    "num_workers = 4\n",
    "num_classes = 3\n",
    "\n",
    "num_epochs = 1  # 학습할 epoch의 수\n",
    "lr = 0.0001\n",
    "lr_decay_step = 10\n",
    "criterion_name = 'cross_entropy' # loss의 이름\n",
    "\n",
    "train_log_interval = 20  # logging할 iteration의 주기\n",
    "name = \"02_model_results\"  # 결과를 저장하는 폴더의 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ac411bb-ef74-47a7-8839-9fd4d589b7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaskDataset 만들기\n",
    "class MaskDataset(Dataset):\n",
    "    def __init__(self,img_path,label_path,transform=True):\n",
    "        self.image = self.load_image(img_path)\n",
    "\n",
    "        self.transform = transform\n",
    "        self.label_path = label_path\n",
    "        #self.age, self.gender, self.mask = self.load_label(label_path)\n",
    "        self.label = self.load_label(label_path)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        #image, age, gender, mask = Image.open(self.image[idx]), self.age[idx], self.gender[idx], self.mask[idx]\n",
    "        image, label= Image.open(self.image[idx]), self.label[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        #return image, age, gender, mask\n",
    "        return image, label\n",
    "    def __len__(self):\n",
    "            return len(self.label)\n",
    "\n",
    "    def load_image(self,paths):\n",
    "        img_lst = []\n",
    "        for dic in os.listdir(paths):\n",
    "            if '._' in dic or 'ipynb_checkpoints' in dic:\n",
    "                continue\n",
    "            dir_path = paths + '/'+ dic\n",
    "            for image in os.listdir(dir_path):\n",
    "                if '._' in image or 'ipynb_checkpoints' in image:\n",
    "                    continue\n",
    "                image_path = dir_path + '/' + image \n",
    "                img_lst.append(image_path)\n",
    "        return img_lst\n",
    "    \n",
    "    def load_label(self, paths):\n",
    "        df = pd.read_csv(os.path.join(paths, \"train_with_labels.csv\"))\n",
    "        #return df['age'], df['gender'], df['mask']\n",
    "        return df['age']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60f5e069-e614-4b4a-b7b8-37657bbdd8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터로더 만들기\n",
    "def getDataloader(dataset, train_idx, valid_idx, batch_size, num_workers):\n",
    "    # 인자로 전달받은 dataset에서 train_idx에 해당하는 Subset 추출\n",
    "    train_set = torch.utils.data.Subset(dataset,\n",
    "                                        indices=train_idx)\n",
    "    # 인자로 전달받은 dataset에서 valid_idx에 해당하는 Subset 추출\n",
    "    val_set = torch.utils.data.Subset(dataset,\n",
    "                                      indices=valid_idx)\n",
    "    \n",
    "    # 추출된 Train Subset으로 DataLoader 생성\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=True,\n",
    "        shuffle=True\n",
    "    )\n",
    "    # 추출된 Valid Subset으로 DataLoader 생성\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_set,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=True,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # 생성한 DataLoader 반환\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5a8c570-a91a-4585-8547-b3bbd9d1b895",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/opt/ml/input/data/train/images'\n",
    "label_path = '/opt/ml/input/data/train'\n",
    "transform = T.Compose([\n",
    "    T.Resize((224,224)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "]) \n",
    "DATA = MaskDataset(img_path,label_path,transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3478a518-b843-48cf-9d6b-590f05d03665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네트워크 필요 입력 채널 개수 3\n",
      "네트워크 출력 채널 개수 (예측 class type 개수) 1000\n"
     ]
    }
   ],
   "source": [
    "resnet18 = torchvision.models.resnet18(pretrained=True)\n",
    "print(\"네트워크 필요 입력 채널 개수\", resnet18.conv1.weight.shape[1])\n",
    "print(\"네트워크 출력 채널 개수 (예측 class type 개수)\", resnet18.fc.weight.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cb61430-6c98-44d4-9b5b-1d862407c56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "target_model = resnet18\n",
    "\n",
    "os.makedirs(os.path.join(os.getcwd(), 'results', name), exist_ok=True)\n",
    "\n",
    "# 5-fold Stratified KFold 5개의 fold를 형성하고 5번 Cross Validation을 진행합니다.\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "counter = 0\n",
    "patience = 10\n",
    "accumulation_steps = 2\n",
    "best_val_acc = 0\n",
    "best_val_loss = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c35c474-97ce-4739-b915-ddd293b80485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/1](20/945) || training loss 8.786 || training accuracy 0.00% || lr [0.0001]\n",
      "Epoch[0/1](40/945) || training loss 7.614 || training accuracy 1.56% || lr [0.0001]\n",
      "Epoch[0/1](60/945) || training loss 6.694 || training accuracy 2.81% || lr [0.0001]\n",
      "Epoch[0/1](80/945) || training loss 5.831 || training accuracy 6.56% || lr [0.0001]\n",
      "Epoch[0/1](100/945) || training loss 4.841 || training accuracy 10.62% || lr [0.0001]\n",
      "Epoch[0/1](120/945) || training loss 4.235 || training accuracy 12.50% || lr [0.0001]\n",
      "Epoch[0/1](140/945) || training loss 3.873 || training accuracy 13.75% || lr [0.0001]\n",
      "Epoch[0/1](160/945) || training loss 3.645 || training accuracy 10.00% || lr [0.0001]\n",
      "Epoch[0/1](180/945) || training loss 3.342 || training accuracy 12.81% || lr [0.0001]\n",
      "Epoch[0/1](200/945) || training loss 3.359 || training accuracy 13.44% || lr [0.0001]\n",
      "Epoch[0/1](220/945) || training loss 3.158 || training accuracy 20.00% || lr [0.0001]\n",
      "Epoch[0/1](240/945) || training loss 3.314 || training accuracy 17.19% || lr [0.0001]\n",
      "Epoch[0/1](260/945) || training loss 3.171 || training accuracy 18.12% || lr [0.0001]\n",
      "Epoch[0/1](280/945) || training loss 3.183 || training accuracy 17.81% || lr [0.0001]\n",
      "Epoch[0/1](300/945) || training loss 3.041 || training accuracy 23.44% || lr [0.0001]\n",
      "Epoch[0/1](320/945) || training loss 3.023 || training accuracy 19.38% || lr [0.0001]\n",
      "Epoch[0/1](340/945) || training loss 3.077 || training accuracy 17.81% || lr [0.0001]\n",
      "Epoch[0/1](360/945) || training loss 2.987 || training accuracy 18.44% || lr [0.0001]\n",
      "Epoch[0/1](380/945) || training loss 2.975 || training accuracy 17.19% || lr [0.0001]\n",
      "Epoch[0/1](400/945) || training loss 2.991 || training accuracy 18.44% || lr [0.0001]\n",
      "Epoch[0/1](420/945) || training loss 2.983 || training accuracy 20.00% || lr [0.0001]\n",
      "Epoch[0/1](440/945) || training loss 2.805 || training accuracy 21.88% || lr [0.0001]\n",
      "Epoch[0/1](460/945) || training loss 2.99 || training accuracy 20.94% || lr [0.0001]\n",
      "Epoch[0/1](480/945) || training loss 2.883 || training accuracy 21.88% || lr [0.0001]\n",
      "Epoch[0/1](500/945) || training loss 2.843 || training accuracy 23.12% || lr [0.0001]\n",
      "Epoch[0/1](520/945) || training loss 2.729 || training accuracy 26.25% || lr [0.0001]\n",
      "Epoch[0/1](540/945) || training loss 2.774 || training accuracy 25.00% || lr [0.0001]\n",
      "Epoch[0/1](560/945) || training loss 2.634 || training accuracy 26.56% || lr [0.0001]\n",
      "Epoch[0/1](580/945) || training loss 2.627 || training accuracy 25.31% || lr [0.0001]\n",
      "Epoch[0/1](600/945) || training loss 2.723 || training accuracy 25.94% || lr [0.0001]\n",
      "Epoch[0/1](620/945) || training loss 2.651 || training accuracy 32.19% || lr [0.0001]\n",
      "Epoch[0/1](640/945) || training loss 2.712 || training accuracy 25.62% || lr [0.0001]\n",
      "Epoch[0/1](660/945) || training loss 2.473 || training accuracy 33.44% || lr [0.0001]\n",
      "Epoch[0/1](680/945) || training loss 2.473 || training accuracy 34.06% || lr [0.0001]\n",
      "Epoch[0/1](700/945) || training loss 2.383 || training accuracy 33.44% || lr [0.0001]\n",
      "Epoch[0/1](720/945) || training loss 2.446 || training accuracy 33.12% || lr [0.0001]\n",
      "Epoch[0/1](740/945) || training loss 2.391 || training accuracy 34.69% || lr [0.0001]\n",
      "Epoch[0/1](760/945) || training loss 2.274 || training accuracy 39.38% || lr [0.0001]\n",
      "Epoch[0/1](780/945) || training loss 2.291 || training accuracy 39.38% || lr [0.0001]\n",
      "Epoch[0/1](800/945) || training loss 2.139 || training accuracy 41.25% || lr [0.0001]\n",
      "Epoch[0/1](820/945) || training loss 2.226 || training accuracy 37.19% || lr [0.0001]\n",
      "Epoch[0/1](840/945) || training loss 2.248 || training accuracy 36.56% || lr [0.0001]\n",
      "Epoch[0/1](860/945) || training loss 2.215 || training accuracy 38.12% || lr [0.0001]\n",
      "Epoch[0/1](880/945) || training loss 2.057 || training accuracy 44.69% || lr [0.0001]\n",
      "Epoch[0/1](900/945) || training loss 2.133 || training accuracy 43.12% || lr [0.0001]\n",
      "Epoch[0/1](920/945) || training loss 2.182 || training accuracy 36.88% || lr [0.0001]\n",
      "Epoch[0/1](940/945) || training loss 1.978 || training accuracy 44.38% || lr [0.0001]\n",
      "Calculating validation results...\n",
      "New best model for val accuracy! saving the model..\n",
      "[Val] acc : 8.97%, loss:  4.1 || best acc : 8.97%, best loss:  4.1\n",
      "Epoch[0/1](20/945) || training loss 2.185 || training accuracy 45.62% || lr [0.0001]\n",
      "Epoch[0/1](40/945) || training loss 2.233 || training accuracy 41.88% || lr [0.0001]\n",
      "Epoch[0/1](60/945) || training loss 2.124 || training accuracy 46.56% || lr [0.0001]\n",
      "Epoch[0/1](80/945) || training loss 1.952 || training accuracy 49.06% || lr [0.0001]\n",
      "Epoch[0/1](100/945) || training loss 1.856 || training accuracy 53.12% || lr [0.0001]\n",
      "Epoch[0/1](120/945) || training loss 1.862 || training accuracy 50.00% || lr [0.0001]\n",
      "Epoch[0/1](140/945) || training loss 1.764 || training accuracy 54.37% || lr [0.0001]\n",
      "Epoch[0/1](160/945) || training loss 1.88 || training accuracy 50.00% || lr [0.0001]\n",
      "Epoch[0/1](180/945) || training loss 1.766 || training accuracy 56.25% || lr [0.0001]\n",
      "Epoch[0/1](200/945) || training loss 1.94 || training accuracy 46.88% || lr [0.0001]\n",
      "Epoch[0/1](220/945) || training loss 1.752 || training accuracy 53.44% || lr [0.0001]\n",
      "Epoch[0/1](240/945) || training loss 1.818 || training accuracy 51.88% || lr [0.0001]\n",
      "Epoch[0/1](260/945) || training loss 1.787 || training accuracy 53.75% || lr [0.0001]\n",
      "Epoch[0/1](280/945) || training loss 1.66 || training accuracy 53.44% || lr [0.0001]\n",
      "Epoch[0/1](300/945) || training loss 1.728 || training accuracy 57.81% || lr [0.0001]\n",
      "Epoch[0/1](320/945) || training loss 1.816 || training accuracy 52.81% || lr [0.0001]\n",
      "Epoch[0/1](340/945) || training loss 1.654 || training accuracy 59.06% || lr [0.0001]\n",
      "Epoch[0/1](360/945) || training loss 1.608 || training accuracy 58.44% || lr [0.0001]\n",
      "Epoch[0/1](380/945) || training loss 1.423 || training accuracy 61.88% || lr [0.0001]\n",
      "Epoch[0/1](400/945) || training loss 1.564 || training accuracy 55.31% || lr [0.0001]\n",
      "Epoch[0/1](420/945) || training loss 1.51 || training accuracy 58.75% || lr [0.0001]\n",
      "Epoch[0/1](440/945) || training loss 1.445 || training accuracy 61.88% || lr [0.0001]\n",
      "Epoch[0/1](460/945) || training loss  1.3 || training accuracy 67.19% || lr [0.0001]\n",
      "Epoch[0/1](480/945) || training loss 1.319 || training accuracy 61.56% || lr [0.0001]\n",
      "Epoch[0/1](500/945) || training loss 1.163 || training accuracy 68.12% || lr [0.0001]\n",
      "Epoch[0/1](520/945) || training loss 1.367 || training accuracy 59.06% || lr [0.0001]\n",
      "Epoch[0/1](540/945) || training loss 1.391 || training accuracy 60.31% || lr [0.0001]\n",
      "Epoch[0/1](560/945) || training loss 1.181 || training accuracy 67.19% || lr [0.0001]\n",
      "Epoch[0/1](580/945) || training loss 1.152 || training accuracy 69.06% || lr [0.0001]\n",
      "Epoch[0/1](600/945) || training loss 1.245 || training accuracy 66.25% || lr [0.0001]\n",
      "Epoch[0/1](620/945) || training loss 1.156 || training accuracy 69.38% || lr [0.0001]\n",
      "Epoch[0/1](640/945) || training loss 1.146 || training accuracy 67.19% || lr [0.0001]\n",
      "Epoch[0/1](660/945) || training loss 1.094 || training accuracy 72.50% || lr [0.0001]\n",
      "Epoch[0/1](680/945) || training loss 1.187 || training accuracy 66.25% || lr [0.0001]\n",
      "Epoch[0/1](700/945) || training loss 1.162 || training accuracy 68.75% || lr [0.0001]\n",
      "Epoch[0/1](720/945) || training loss 1.118 || training accuracy 68.75% || lr [0.0001]\n",
      "Epoch[0/1](740/945) || training loss 1.044 || training accuracy 71.25% || lr [0.0001]\n",
      "Epoch[0/1](760/945) || training loss 0.9976 || training accuracy 74.06% || lr [0.0001]\n",
      "Epoch[0/1](780/945) || training loss 1.141 || training accuracy 66.88% || lr [0.0001]\n",
      "Epoch[0/1](800/945) || training loss 1.004 || training accuracy 71.88% || lr [0.0001]\n",
      "Epoch[0/1](820/945) || training loss 1.005 || training accuracy 73.44% || lr [0.0001]\n",
      "Epoch[0/1](840/945) || training loss 0.9432 || training accuracy 75.31% || lr [0.0001]\n",
      "Epoch[0/1](860/945) || training loss 0.96 || training accuracy 75.62% || lr [0.0001]\n",
      "Epoch[0/1](880/945) || training loss 0.8719 || training accuracy 76.25% || lr [0.0001]\n",
      "Epoch[0/1](900/945) || training loss 0.9851 || training accuracy 72.81% || lr [0.0001]\n",
      "Epoch[0/1](920/945) || training loss 0.9786 || training accuracy 73.44% || lr [0.0001]\n",
      "Epoch[0/1](940/945) || training loss 0.7251 || training accuracy 81.88% || lr [0.0001]\n",
      "Calculating validation results...\n",
      "New best model for val accuracy! saving the model..\n",
      "[Val] acc : 34.52%, loss:  2.4 || best acc : 34.52%, best loss:  2.4\n",
      "Epoch[0/1](20/945) || training loss 0.9159 || training accuracy 75.31% || lr [0.0001]\n",
      "Epoch[0/1](40/945) || training loss  1.2 || training accuracy 71.56% || lr [0.0001]\n",
      "Epoch[0/1](60/945) || training loss 1.088 || training accuracy 72.50% || lr [0.0001]\n",
      "Epoch[0/1](80/945) || training loss 0.9882 || training accuracy 75.31% || lr [0.0001]\n",
      "Epoch[0/1](100/945) || training loss 0.9325 || training accuracy 76.88% || lr [0.0001]\n",
      "Epoch[0/1](120/945) || training loss 0.941 || training accuracy 76.25% || lr [0.0001]\n",
      "Epoch[0/1](140/945) || training loss 0.9091 || training accuracy 74.38% || lr [0.0001]\n",
      "Epoch[0/1](160/945) || training loss 0.7747 || training accuracy 81.88% || lr [0.0001]\n",
      "Epoch[0/1](180/945) || training loss 0.8866 || training accuracy 76.56% || lr [0.0001]\n",
      "Epoch[0/1](200/945) || training loss 0.8843 || training accuracy 74.06% || lr [0.0001]\n",
      "Epoch[0/1](220/945) || training loss 0.8203 || training accuracy 80.31% || lr [0.0001]\n",
      "Epoch[0/1](240/945) || training loss 0.7662 || training accuracy 80.31% || lr [0.0001]\n",
      "Epoch[0/1](260/945) || training loss 0.7922 || training accuracy 80.62% || lr [0.0001]\n",
      "Epoch[0/1](280/945) || training loss 0.7022 || training accuracy 82.50% || lr [0.0001]\n",
      "Epoch[0/1](300/945) || training loss 0.7785 || training accuracy 80.00% || lr [0.0001]\n",
      "Epoch[0/1](320/945) || training loss 0.7925 || training accuracy 78.44% || lr [0.0001]\n",
      "Epoch[0/1](340/945) || training loss 0.7823 || training accuracy 80.00% || lr [0.0001]\n",
      "Epoch[0/1](360/945) || training loss 0.6075 || training accuracy 84.06% || lr [0.0001]\n",
      "Epoch[0/1](380/945) || training loss 0.5879 || training accuracy 84.06% || lr [0.0001]\n",
      "Epoch[0/1](400/945) || training loss 0.5819 || training accuracy 86.88% || lr [0.0001]\n",
      "Epoch[0/1](420/945) || training loss 0.7244 || training accuracy 80.94% || lr [0.0001]\n",
      "Epoch[0/1](440/945) || training loss 0.6486 || training accuracy 84.69% || lr [0.0001]\n",
      "Epoch[0/1](460/945) || training loss 0.5661 || training accuracy 86.56% || lr [0.0001]\n",
      "Epoch[0/1](480/945) || training loss 0.6161 || training accuracy 83.75% || lr [0.0001]\n",
      "Epoch[0/1](500/945) || training loss 0.5799 || training accuracy 85.31% || lr [0.0001]\n",
      "Epoch[0/1](520/945) || training loss 0.5844 || training accuracy 85.62% || lr [0.0001]\n",
      "Epoch[0/1](540/945) || training loss 0.6034 || training accuracy 85.31% || lr [0.0001]\n",
      "Epoch[0/1](560/945) || training loss 0.5601 || training accuracy 85.94% || lr [0.0001]\n",
      "Epoch[0/1](580/945) || training loss 0.6321 || training accuracy 84.69% || lr [0.0001]\n",
      "Epoch[0/1](600/945) || training loss 0.5479 || training accuracy 87.50% || lr [0.0001]\n",
      "Epoch[0/1](620/945) || training loss 0.5716 || training accuracy 85.00% || lr [0.0001]\n",
      "Epoch[0/1](640/945) || training loss 0.5696 || training accuracy 83.75% || lr [0.0001]\n",
      "Epoch[0/1](660/945) || training loss 0.4809 || training accuracy 88.12% || lr [0.0001]\n",
      "Epoch[0/1](680/945) || training loss 0.5001 || training accuracy 87.19% || lr [0.0001]\n",
      "Epoch[0/1](700/945) || training loss 0.4772 || training accuracy 86.88% || lr [0.0001]\n",
      "Epoch[0/1](720/945) || training loss 0.4628 || training accuracy 85.00% || lr [0.0001]\n",
      "Epoch[0/1](740/945) || training loss 0.4933 || training accuracy 86.25% || lr [0.0001]\n",
      "Epoch[0/1](760/945) || training loss 0.4176 || training accuracy 87.19% || lr [0.0001]\n",
      "Epoch[0/1](780/945) || training loss 0.3994 || training accuracy 89.38% || lr [0.0001]\n",
      "Epoch[0/1](800/945) || training loss 0.4281 || training accuracy 89.69% || lr [0.0001]\n",
      "Epoch[0/1](820/945) || training loss 0.3762 || training accuracy 90.31% || lr [0.0001]\n",
      "Epoch[0/1](840/945) || training loss 0.3887 || training accuracy 90.94% || lr [0.0001]\n",
      "Epoch[0/1](860/945) || training loss 0.4099 || training accuracy 90.31% || lr [0.0001]\n",
      "Epoch[0/1](880/945) || training loss 0.4059 || training accuracy 91.88% || lr [0.0001]\n",
      "Epoch[0/1](900/945) || training loss 0.3312 || training accuracy 92.19% || lr [0.0001]\n",
      "Epoch[0/1](920/945) || training loss 0.4201 || training accuracy 89.38% || lr [0.0001]\n",
      "Epoch[0/1](940/945) || training loss 0.4508 || training accuracy 89.69% || lr [0.0001]\n",
      "Calculating validation results...\n",
      "New best model for val accuracy! saving the model..\n",
      "[Val] acc : 76.19%, loss:  0.8 || best acc : 76.19%, best loss:  0.8\n",
      "Epoch[0/1](20/945) || training loss 0.4172 || training accuracy 88.75% || lr [0.0001]\n",
      "Epoch[0/1](40/945) || training loss 0.4243 || training accuracy 88.12% || lr [0.0001]\n",
      "Epoch[0/1](60/945) || training loss 0.4296 || training accuracy 88.44% || lr [0.0001]\n",
      "Epoch[0/1](80/945) || training loss 0.449 || training accuracy 88.75% || lr [0.0001]\n",
      "Epoch[0/1](100/945) || training loss 0.4006 || training accuracy 90.62% || lr [0.0001]\n",
      "Epoch[0/1](120/945) || training loss 0.4209 || training accuracy 88.12% || lr [0.0001]\n",
      "Epoch[0/1](140/945) || training loss 0.3837 || training accuracy 90.94% || lr [0.0001]\n",
      "Epoch[0/1](160/945) || training loss 0.4005 || training accuracy 91.25% || lr [0.0001]\n",
      "Epoch[0/1](180/945) || training loss 0.383 || training accuracy 89.69% || lr [0.0001]\n",
      "Epoch[0/1](200/945) || training loss 0.3595 || training accuracy 90.31% || lr [0.0001]\n",
      "Epoch[0/1](220/945) || training loss 0.3562 || training accuracy 89.69% || lr [0.0001]\n",
      "Epoch[0/1](240/945) || training loss 0.3377 || training accuracy 92.19% || lr [0.0001]\n",
      "Epoch[0/1](260/945) || training loss 0.322 || training accuracy 92.19% || lr [0.0001]\n",
      "Epoch[0/1](280/945) || training loss 0.3897 || training accuracy 90.31% || lr [0.0001]\n",
      "Epoch[0/1](300/945) || training loss 0.4056 || training accuracy 91.88% || lr [0.0001]\n",
      "Epoch[0/1](320/945) || training loss 0.2513 || training accuracy 94.06% || lr [0.0001]\n",
      "Epoch[0/1](340/945) || training loss 0.3143 || training accuracy 92.19% || lr [0.0001]\n",
      "Epoch[0/1](360/945) || training loss 0.327 || training accuracy 93.12% || lr [0.0001]\n",
      "Epoch[0/1](380/945) || training loss 0.2882 || training accuracy 92.81% || lr [0.0001]\n",
      "Epoch[0/1](400/945) || training loss 0.3464 || training accuracy 90.62% || lr [0.0001]\n",
      "Epoch[0/1](420/945) || training loss 0.283 || training accuracy 93.44% || lr [0.0001]\n",
      "Epoch[0/1](440/945) || training loss 0.3141 || training accuracy 90.94% || lr [0.0001]\n",
      "Epoch[0/1](460/945) || training loss 0.2805 || training accuracy 92.19% || lr [0.0001]\n",
      "Epoch[0/1](480/945) || training loss 0.3821 || training accuracy 91.56% || lr [0.0001]\n",
      "Epoch[0/1](500/945) || training loss 0.3201 || training accuracy 91.88% || lr [0.0001]\n",
      "Epoch[0/1](520/945) || training loss 0.316 || training accuracy 89.69% || lr [0.0001]\n",
      "Epoch[0/1](540/945) || training loss 0.2918 || training accuracy 91.25% || lr [0.0001]\n",
      "Epoch[0/1](560/945) || training loss 0.326 || training accuracy 91.88% || lr [0.0001]\n",
      "Epoch[0/1](580/945) || training loss 0.3689 || training accuracy 90.00% || lr [0.0001]\n",
      "Epoch[0/1](600/945) || training loss 0.245 || training accuracy 94.38% || lr [0.0001]\n",
      "Epoch[0/1](620/945) || training loss 0.2619 || training accuracy 93.75% || lr [0.0001]\n",
      "Epoch[0/1](640/945) || training loss 0.2762 || training accuracy 92.50% || lr [0.0001]\n",
      "Epoch[0/1](660/945) || training loss 0.2548 || training accuracy 94.69% || lr [0.0001]\n",
      "Epoch[0/1](680/945) || training loss 0.2176 || training accuracy 95.94% || lr [0.0001]\n",
      "Epoch[0/1](700/945) || training loss 0.3143 || training accuracy 91.56% || lr [0.0001]\n",
      "Epoch[0/1](720/945) || training loss 0.2552 || training accuracy 93.44% || lr [0.0001]\n",
      "Epoch[0/1](740/945) || training loss 0.2654 || training accuracy 94.06% || lr [0.0001]\n",
      "Epoch[0/1](760/945) || training loss 0.2661 || training accuracy 92.81% || lr [0.0001]\n",
      "Epoch[0/1](780/945) || training loss 0.2692 || training accuracy 93.44% || lr [0.0001]\n",
      "Epoch[0/1](800/945) || training loss 0.282 || training accuracy 91.56% || lr [0.0001]\n",
      "Epoch[0/1](820/945) || training loss 0.2292 || training accuracy 94.38% || lr [0.0001]\n",
      "Epoch[0/1](840/945) || training loss 0.2336 || training accuracy 94.06% || lr [0.0001]\n",
      "Epoch[0/1](860/945) || training loss 0.198 || training accuracy 94.69% || lr [0.0001]\n",
      "Epoch[0/1](880/945) || training loss 0.2276 || training accuracy 94.38% || lr [0.0001]\n",
      "Epoch[0/1](900/945) || training loss 0.1955 || training accuracy 95.94% || lr [0.0001]\n",
      "Epoch[0/1](920/945) || training loss 0.2192 || training accuracy 95.62% || lr [0.0001]\n",
      "Epoch[0/1](940/945) || training loss 0.2367 || training accuracy 95.31% || lr [0.0001]\n",
      "Calculating validation results...\n",
      "New best model for val accuracy! saving the model..\n",
      "[Val] acc : 87.72%, loss: 0.41 || best acc : 87.72%, best loss: 0.41\n",
      "Epoch[0/1](20/945) || training loss 0.3179 || training accuracy 90.31% || lr [0.0001]\n",
      "Epoch[0/1](40/945) || training loss 0.3235 || training accuracy 90.31% || lr [0.0001]\n",
      "Epoch[0/1](60/945) || training loss 0.27 || training accuracy 92.19% || lr [0.0001]\n",
      "Epoch[0/1](80/945) || training loss 0.2449 || training accuracy 94.69% || lr [0.0001]\n",
      "Epoch[0/1](100/945) || training loss 0.2594 || training accuracy 93.75% || lr [0.0001]\n",
      "Epoch[0/1](120/945) || training loss 0.2512 || training accuracy 92.81% || lr [0.0001]\n",
      "Epoch[0/1](140/945) || training loss 0.2622 || training accuracy 91.88% || lr [0.0001]\n",
      "Epoch[0/1](160/945) || training loss 0.2011 || training accuracy 95.31% || lr [0.0001]\n",
      "Epoch[0/1](180/945) || training loss 0.3006 || training accuracy 91.56% || lr [0.0001]\n",
      "Epoch[0/1](200/945) || training loss 0.2049 || training accuracy 94.69% || lr [0.0001]\n",
      "Epoch[0/1](220/945) || training loss 0.2638 || training accuracy 93.12% || lr [0.0001]\n",
      "Epoch[0/1](240/945) || training loss 0.2235 || training accuracy 92.81% || lr [0.0001]\n",
      "Epoch[0/1](260/945) || training loss 0.189 || training accuracy 97.19% || lr [0.0001]\n",
      "Epoch[0/1](280/945) || training loss 0.2302 || training accuracy 93.75% || lr [0.0001]\n",
      "Epoch[0/1](300/945) || training loss 0.2043 || training accuracy 94.69% || lr [0.0001]\n",
      "Epoch[0/1](320/945) || training loss 0.176 || training accuracy 95.94% || lr [0.0001]\n",
      "Epoch[0/1](340/945) || training loss 0.145 || training accuracy 97.81% || lr [0.0001]\n",
      "Epoch[0/1](360/945) || training loss 0.1841 || training accuracy 94.69% || lr [0.0001]\n",
      "Epoch[0/1](380/945) || training loss 0.2156 || training accuracy 93.75% || lr [0.0001]\n",
      "Epoch[0/1](400/945) || training loss 0.1834 || training accuracy 95.31% || lr [0.0001]\n",
      "Epoch[0/1](420/945) || training loss 0.2351 || training accuracy 93.12% || lr [0.0001]\n",
      "Epoch[0/1](440/945) || training loss 0.1397 || training accuracy 97.81% || lr [0.0001]\n",
      "Epoch[0/1](460/945) || training loss 0.1685 || training accuracy 96.56% || lr [0.0001]\n",
      "Epoch[0/1](480/945) || training loss 0.2096 || training accuracy 94.69% || lr [0.0001]\n",
      "Epoch[0/1](500/945) || training loss 0.1786 || training accuracy 96.56% || lr [0.0001]\n",
      "Epoch[0/1](520/945) || training loss 0.1714 || training accuracy 95.94% || lr [0.0001]\n",
      "Epoch[0/1](540/945) || training loss 0.2053 || training accuracy 95.94% || lr [0.0001]\n",
      "Epoch[0/1](560/945) || training loss 0.1901 || training accuracy 96.88% || lr [0.0001]\n",
      "Epoch[0/1](580/945) || training loss 0.1434 || training accuracy 97.19% || lr [0.0001]\n",
      "Epoch[0/1](600/945) || training loss 0.1432 || training accuracy 97.50% || lr [0.0001]\n",
      "Epoch[0/1](620/945) || training loss 0.1546 || training accuracy 96.56% || lr [0.0001]\n",
      "Epoch[0/1](640/945) || training loss 0.1992 || training accuracy 95.00% || lr [0.0001]\n",
      "Epoch[0/1](660/945) || training loss 0.1591 || training accuracy 96.25% || lr [0.0001]\n",
      "Epoch[0/1](680/945) || training loss 0.2353 || training accuracy 92.81% || lr [0.0001]\n",
      "Epoch[0/1](700/945) || training loss 0.1671 || training accuracy 97.19% || lr [0.0001]\n",
      "Epoch[0/1](720/945) || training loss 0.2016 || training accuracy 94.06% || lr [0.0001]\n",
      "Epoch[0/1](740/945) || training loss 0.172 || training accuracy 95.62% || lr [0.0001]\n",
      "Epoch[0/1](760/945) || training loss 0.1582 || training accuracy 95.62% || lr [0.0001]\n",
      "Epoch[0/1](780/945) || training loss 0.1945 || training accuracy 94.69% || lr [0.0001]\n",
      "Epoch[0/1](800/945) || training loss 0.1824 || training accuracy 95.31% || lr [0.0001]\n",
      "Epoch[0/1](820/945) || training loss 0.1919 || training accuracy 95.31% || lr [0.0001]\n",
      "Epoch[0/1](840/945) || training loss 0.2059 || training accuracy 94.69% || lr [0.0001]\n",
      "Epoch[0/1](860/945) || training loss 0.1991 || training accuracy 95.31% || lr [0.0001]\n",
      "Epoch[0/1](880/945) || training loss 0.2072 || training accuracy 93.75% || lr [0.0001]\n",
      "Epoch[0/1](900/945) || training loss 0.1517 || training accuracy 95.62% || lr [0.0001]\n",
      "Epoch[0/1](920/945) || training loss 0.1588 || training accuracy 95.62% || lr [0.0001]\n",
      "Epoch[0/1](940/945) || training loss 0.1674 || training accuracy 95.31% || lr [0.0001]\n",
      "Calculating validation results...\n",
      "New best model for val accuracy! saving the model..\n",
      "[Val] acc : 92.94%, loss: 0.25 || best acc : 92.94%, best loss: 0.25\n"
     ]
    }
   ],
   "source": [
    "# 학습 진행\n",
    "for i, (train_idx, valid_idx) in enumerate(skf.split(DATA.image, DATA.label)):\n",
    "    \n",
    "    # 생성한 Train, Valid Index를 getDataloader 함수에 전달해 train/valid DataLoader를 생성합니다.\n",
    "    # 생성한 train, valid DataLoader로 이전과 같이 모델 학습을 진행합니다. \n",
    "    train_loader, val_loader = getDataloader(DATA, train_idx, valid_idx, batch_size, num_workers)\n",
    "\n",
    "    # -- model\n",
    "    model = resnet18\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    # -- loss & metric\n",
    "    criterion = torch.nn.CrossEntropyLoss() # 분류 학습 때 많이 사용되는 Cross entropy loss를 objective function으로 사용 - https://en.wikipedia.org/wiki/Cross_entropy\n",
    "    optimizer = torch.optim.Adam(target_model.parameters(), lr=lr) # weight 업데이트를 위한 optimizer를 Adam으로 사용함\n",
    "\n",
    "    scheduler = StepLR(optimizer, lr_decay_step, gamma=0.5)\n",
    "\n",
    "    # -- logging\n",
    "    logger = SummaryWriter(log_dir=f\"results/cv{i}_{name}\")\n",
    "    for epoch in range(num_epochs):\n",
    "        # train loop\n",
    "        model.train()\n",
    "        loss_value = 0\n",
    "        matches = 0\n",
    "        for idx, train_batch in enumerate(train_loader):\n",
    "            inputs, labels = train_batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outs = model(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "            loss = criterion(outs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "             # -- Gradient Accumulation\n",
    "            if (idx+1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            loss_value += loss.item()\n",
    "            matches += (preds == labels).sum().item()\n",
    "            if (idx + 1) % train_log_interval == 0:\n",
    "                train_loss = loss_value / train_log_interval\n",
    "                train_acc = matches / batch_size / train_log_interval\n",
    "                current_lr = scheduler.get_last_lr()\n",
    "                print(\n",
    "                    f\"Epoch[{epoch}/{num_epochs}]({idx + 1}/{len(train_loader)}) || \"\n",
    "                    f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n",
    "                )\n",
    "\n",
    "                loss_value = 0\n",
    "                matches = 0\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # val loop\n",
    "        with torch.no_grad():\n",
    "            print(\"Calculating validation results...\")\n",
    "            model.eval()\n",
    "            val_loss_items = []\n",
    "            val_acc_items = []\n",
    "            for val_batch in val_loader:\n",
    "                inputs, labels = val_batch\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outs = model(inputs)\n",
    "                preds = torch.argmax(outs, dim=-1)\n",
    "\n",
    "                loss_item = criterion(outs, labels).item()\n",
    "                acc_item = (labels == preds).sum().item()\n",
    "                val_loss_items.append(loss_item)\n",
    "                val_acc_items.append(acc_item)\n",
    "\n",
    "            val_loss = np.sum(val_loss_items) / len(val_loader)\n",
    "            val_acc = np.sum(val_acc_items) / len(valid_idx)\n",
    "\n",
    "            # Callback1: validation accuracy가 향상될수록 모델을 저장합니다.\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "            if val_acc > best_val_acc:\n",
    "                print(\"New best model for val accuracy! saving the model..\")\n",
    "                torch.save(model.state_dict(), f\"results/{name}/{epoch:03}_accuracy_{val_acc:4.2%}.ckpt\")\n",
    "                best_val_acc = val_acc\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "            # Callback2: patience 횟수 동안 성능 향상이 없을 경우 학습을 종료시킵니다.\n",
    "            if counter > patience:\n",
    "                print(\"Early Stopping...\")\n",
    "                break\n",
    "\n",
    "\n",
    "            print(\n",
    "                f\"[Val] acc : {val_acc:4.2%}, loss: {val_loss:4.2} || \"\n",
    "                f\"best acc : {best_val_acc:4.2%}, best loss: {best_val_loss:4.2}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1c5806-4278-428d-80c3-87f08e9b7582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
